import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

logger = logging.getLogger(__name__)

def run_inference(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompt: str, max_new_tokens: int = 50) -> str:
    """
    Runs inference using the provided model, tokenizer, and prompt.

    Args:
        model: The loaded Hugging Face Causal LM model.
        tokenizer: The loaded Hugging Face tokenizer.
        prompt: The input text prompt.
        max_new_tokens: The maximum number of new tokens to generate.

    Returns:
        The generated text string (including the prompt if not skipped).
        Returns None if inference fails.
    """
    try:
        logger.info(f"Running inference with prompt: '{prompt[:100]}...' ({len(prompt)} chars)")
        # Ensure model and tokenizer are valid
        if not model or not tokenizer:
            logger.error("Model or tokenizer is invalid.")
            return None

        # Determine device (use GPU if available)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        logger.info(f"Using device: {device}")

        # Tokenize the input prompt
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

        # Generate output tokens
        # Use generate method for more control over generation parameters
        # Pad token id might be needed if batching, but here we process one prompt
        # Setting pad_token_id to eos_token_id is a common practice for open-ended generation
        pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id

        with torch.no_grad(): # Disable gradient calculations for inference
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=True, # Use sampling for more varied output
                top_k=50,
                top_p=0.95
            )

        # Decode the generated token IDs back to text
        # Skip special tokens to avoid printing <|endoftext|> etc.
        # We decode the full output, including the prompt part generated by the model.
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        logger.info(f"Inference completed. Generated text length: {len(generated_text)}")
        return generated_text

    except Exception as e:
        logger.error(f"Error during inference: {e}", exc_info=True)
        return None

# Example Usage
if __name__ == '__main__':
    from seel.model_loader import load_model_and_tokenizer
    import time

    model_id = "distilgpt2"
    print(f"\n--- Inference Runner Test ({model_id}) ---")
    print("Loading model (this might take time)...")
    start_load = time.time()
    model, tokenizer, _, _ = load_model_and_tokenizer(model_id)
    load_time = time.time() - start_load
    print(f"Model loaded in {load_time:.2f} seconds.")

    if model and tokenizer:
        test_prompt = "Once upon a time,"
        print(f"Running inference for prompt: '{test_prompt}'")
        start_infer = time.time()
        generated_output = run_inference(model, tokenizer, test_prompt, max_new_tokens=30)
        infer_time = time.time() - start_infer

        if generated_output:
            print(f"\nGenerated Text (took {infer_time:.2f}s):")
            print(generated_output)
        else:
            print("Inference failed.")
        print("---------------------------------")
    else:
        print(f"Could not load model {model_id} for inference test.") 